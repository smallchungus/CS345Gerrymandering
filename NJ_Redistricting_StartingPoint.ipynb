{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a3d493-83fb-4965-bdcb-f1054425e86e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geojson in c:\\users\\aqila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: shapely in c:\\users\\aqila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: numpy>=1.14 in c:\\users\\aqila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shapely) (1.26.1)\n",
      "Requirement already satisfied: PyShp in c:\\users\\aqila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\aqila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install geojson\n",
    "!pip install shapely\n",
    "!pip install PyShp\n",
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40a7f250-1f24-4f02-832b-1df2dceaa50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geojson\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "import csv\n",
    "import ast\n",
    "import shapefile as shp\n",
    "from shapely.geometry import Polygon,shape,MultiPolygon\n",
    "import shapely.ops\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1549e7-0856-443a-aedc-df06d9caf33a",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad28b0c5-f5b6-4baf-9295-07dbff55df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isDistrictContiguous(district_num, assignment, contiguity_list, print_isolates=False, ignore_list=[]):\n",
    "    ## input:\n",
    "    ## district_num: the district number\n",
    "    ## assignment: the assignment from precinct to district\n",
    "    ## contiguity_list: the list of neighbors for each precinct, from the csv file\n",
    "    contiguity_list.columns = ['Precinct','Neighbors']\n",
    "    district_graph = nx.Graph() #creates an empty undirected graph\n",
    "    district_nodes = assignment[assignment['District']==district_num]['GEOID20'].tolist()\n",
    "    for i in ignore_list:\n",
    "        try:\n",
    "            district_nodes.remove(i)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    district_graph.add_nodes_from(district_nodes)\n",
    "    for id in district_nodes:\n",
    "        neighbors = ast.literal_eval(contiguity_list[contiguity_list['Precinct']==id]['Neighbors'].values.tolist()[0])\n",
    "        # needed to convert string to list because the csv encodes the list as a string\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor in district_nodes:\n",
    "                district_graph.add_edge(id,neighbor)\n",
    "    if(print_isolates):\n",
    "        print(list(nx.isolates(district_graph)))\n",
    "    return nx.is_connected(district_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "474da28a-35a4-4115-b19e-9439d1044e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistrictPopulations(assignment,data_file, num_district):\n",
    "    population = {}\n",
    "    for i in range (1,num_district+1):\n",
    "        population[i] = data_file[data_file['GEOID20'].isin(assignment[assignment['District']==i]['GEOID20'])]['Total_2020_Total'].sum()\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92c27697-04a7-4169-843b-ce2d98622efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistrictShape(district_id, assignment, boundaries):\n",
    "    list_precincts = assignment[assignment['District']==district_id]['GEOID20']\n",
    "    precinct_shapes = []\n",
    "    for i in list_precincts:\n",
    "        if shape(boundaries[i]).type == 'Polygon':\n",
    "            precinct_shapes.append(Polygon(shape(boundaries[i])))\n",
    "        elif shape(boundaries[i]).type == 'MultiPolygon':\n",
    "            precinct_shapes.append(MultiPolygon(shape(boundaries[i])))      \n",
    "    district_shape = shapely.ops.unary_union(precinct_shapes)\n",
    "    #print(district_shape)\n",
    "    return district_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5e8260f-2f6a-497e-8ff6-51dd9a767180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_compactness(geom): # Polsby-Popper\n",
    "    p = geom.length\n",
    "    a = geom.area    \n",
    "    return (4*np.pi*a)/(p*p)\n",
    "\n",
    "def box_reock_compactness(geom): # Reock on a rectangle bounding box\n",
    "    a = geom.area \n",
    "    bb = geom.bounds # bounds gives you the minimum bounding box (rectangle)\n",
    "    bba = abs(bb[0]-bb[2])*abs(bb[1]-bb[3])\n",
    "    return a/bba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959227a9-93f8-47fa-87ef-02037facafb9",
   "metadata": {},
   "source": [
    "# This Notebook will help you get started on NJ\n",
    "The data is in Canvas, you should upload it to your Google Drive first (if using Colab), or local filesystem (if using Jupyter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69143d08-629d-436a-8687-7a5d3bcb5aa3",
   "metadata": {},
   "source": [
    "### This is the current assignment of precinct to congressional districts (12 of them for NJ)\n",
    "Note that the map shown in DRA is slightly different. This is because some precincts are split in the real assignment, and some additional precinct are created to handle special situations such as prisoners and overseas citizens. You can ignore this for the class project and just use the data and functions provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c0c2b03-9b2f-46c5-b619-0709ca555c8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Map_Data/precinct-assignments-congress-nj.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nj_current_assignment \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMap_Data/precinct-assignments-congress-nj.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m nj_current_assignment\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Map_Data/precinct-assignments-congress-nj.csv'"
     ]
    }
   ],
   "source": [
    "nj_current_assignment = pd.read_csv('Map_Data/precinct-assignments-congress-nj.csv')\n",
    "nj_current_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6bce06-2f20-4f82-959e-a95f91e25ba5",
   "metadata": {},
   "source": [
    "### This is the current demographic and voter data\n",
    "The data has a lot of attributes that lists voters of different demographics and parties in different elections. You can look at the data Dictionary on Canvas to get details. For this recitation we will only keep votes from the 2020  presidential election and the total 2020 population counts. You can use additional columns (e.g., Governor's elections results, voting age (VAP) population counts, or the composite Dem/Rep score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13a23f-3b65-4852-a42d-3a87867f3361",
   "metadata": {},
   "outputs": [],
   "source": [
    "nj_precinct_data = pd.read_csv('Map_Data/precinct-data-congress-nj.csv')\n",
    "keepcolumns = ['GEOID20','District','Total_2020_Pres','Dem_2020_Pres','Rep_2020_Pres','Total_2020_Total','White_2020_Total','Hispanic_2020_Total','Black_2020_Total','Asian_2020_Total','Native_2020_Total','Pacific_2020_Total']\n",
    "nj_precinct_data = nj_precinct_data[keepcolumns]\n",
    "nj_precinct_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609e75d5-6d29-4ab2-ab58-39c67f8b79f1",
   "metadata": {},
   "source": [
    "### This is the precinct boundary data (uses shapely)\n",
    "\n",
    "This is data that represents the geography of the districts. It is needed to test for contiguity, or for any districting partitioning method based on geography. The data is in Shapely format. Each district is represented as a set of points that are connected to create the district shape (in the long/lat coordinates). Shapely geometric functions can be used to compare the shapes. These can be quite inefficient to run, so I am also providing you a pre-computed index that, for each district, lists the districts that are contiguous to it. You can see the code to generate the index in Contiguity.ipynb.\n",
    "\n",
    "To manipulate the shapes, cast them into Shapely Polygons (see example below) and you can use the Polygon properties and functions: https://shapely.readthedocs.io/en/stable/reference/shapely.Polygon.html#shapely.Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3dbde7-e314-4fe0-9861-969d96e13645",
   "metadata": {},
   "outputs": [],
   "source": [
    "shpfile = 'Map_Data/nj_vtd_2020_bound/nj_vtd_2020_bound.shp'\n",
    "dbffile = 'Map_Data/nj_vtd_2020_bound/nj_vtd_2020_bound.dbf'\n",
    "shxfile = 'Map_Data/nj_vtd_2020_bound/nj_vtd_2020_bound.shx'\n",
    "\n",
    "\n",
    "shpfile = shp.Reader(shp=shpfile, shx=shxfile, dbf=dbffile)\n",
    "nj_precinct_boundaries={}\n",
    "for sr in shpfile.iterShapeRecords():\n",
    "    geom = sr.shape # get geo bit\n",
    "    rec = sr.record # get db fields\n",
    "    nj_precinct_boundaries[rec[3]]=geom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a473b-0936-4c0e-93cc-850329eaa553",
   "metadata": {},
   "source": [
    "### This is the precinct boundary data \n",
    "\n",
    "This use the contiguity index I have pre-computed using Contiguity.ipynb, that is stored in Contiguity_nj.csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c9f089-caad-45b3-b795-71793bcfedb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nj_contiguity = pd.read_csv('Contiguity_nj.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c4cda-bb3c-4392-9df0-5e075c19c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,13):\n",
    "    print(\"District \"+str(i)+\" \"+str(isDistrictContiguous(12, nj_current_assignment, nj_contiguity)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bad083-6853-462f-be62-38ff2f84e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compactness of the current assignment\n",
    "for district in range(1,13):\n",
    "    print(\"D\"+str(district)+\" PP : \"+str(pp_compactness(getDistrictShape(district,nj_current_assignment,nj_precinct_boundaries))))\n",
    "    print(\"D\"+str(district)+\" BR : \"+str(box_reock_compactness(getDistrictShape(district,nj_current_assignment,nj_precinct_boundaries))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f2ef1-b4be-4c73-b87a-f6e9f6e9b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# District Population of the current assignment\n",
    "print(getDistrictPopulations(nj_current_assignment,nj_precinct_data, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e696388-313f-4090-9035-09a7da258a19",
   "metadata": {},
   "source": [
    "# A simple geographical  redistricting strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ced817-48c2-4b56-8a46-c274e5b9ff7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can create a simple geopgraphical map, like we did for NH. In this case, we have 12 districts, so let's splitting the district in half North/South, and in 6th  East/West. \n",
    "New Hampshire's bounding box is (-75.559614,38.928519,-73.893979,41.357423) (https://anthonylouisdagostino.com/bounding-boxes-for-all-us-states/)\n",
    "So let's start by splitting the state approximately though the middle longitude (-74.72) : everything west of longitude -71.583934 is in odd Districts, everything east is in even Districts. We will use the precinct centroids to assign them. Then we will divide each half per latitude on the ranges  (38.92, 39.3, 39.7, 40.1, 40.5,40.9,41.35)\n",
    "Import the Map to DRA to look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da30c782-b466-47d7-bb37-78b3cad6bd34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nj_longlat_assignment = nj_current_assignment.copy()\n",
    "nj_longlat_assignment['District'] = 0\n",
    "for index, row in nj_longlat_assignment.iterrows():\n",
    "    try:\n",
    "        if shape(nj_precinct_boundaries[row['GEOID20']]).type == 'Polygon':\n",
    "            centroid = Polygon(shape(nj_precinct_boundaries[row['GEOID20']])).centroid\n",
    "        elif shape(nj_precinct_boundaries[row['GEOID20']]).type == 'MultiPolygon':\n",
    "            centroid = MultiPolygon(shape(nj_precinct_boundaries[row['GEOID20']])).centroid\n",
    "        else:\n",
    "            print(shape(nj_precinct_boundaries[row['GEOID20']]).type)\n",
    "            pass\n",
    "        if centroid.x <= -74.72:\n",
    "            if centroid.y <= 39.3:\n",
    "                nj_longlat_assignment.iloc[index,nj_longlat_assignment.columns.get_loc('District')] = 1\n",
    "            elif centroid.y <= 39.7:\n",
    "                nj_longlat_assignment.iloc[index,nj_longlat_assignment.columns.get_loc('District')] = 3\n",
    "            elif centroid.y <= 40.1:\n",
    "                nj_longlat_assignment.iloc[index,nj_longlat_assignment.columns.get_loc('District')] = 5\n",
    "            elif centroid.y <= 40.5:\n",
    "                nj_longlat_assignment.iloc[index,nj_longlat_assignment.columns.get_loc('District')] = 7\n",
    "            elif centroid.y <= 40.9:\n",
    "                nj_longlat_assignment.iloc[index,nj_longlat_assignment.columns.get_loc('District')] = 9\n",
    "            else:\n",
    "                nj_longlat_assignment.iloc[index,nj_longlat_assignment.columns.get_loc('District')] = 11\n",
    "        else:\n",
    "            if centroid.y <= 39.3:\n",
    "                nj_longlat_assignment.iloc[index,nj_longlat_assignment.columns.get_loc('District')] = 2\n",
    "            elif centroid.y <= 39.7:\n",
    "                nj_longlat_assignment.iloc[index,nj_longlat_assignment.columns.get_loc('District')] = 4\n",
    "            elif centroid.y <= 40.1:\n",
    "                nj_longlat_assignment.iloc[index,nj_longlat_assignment.columns.get_loc('District')] = 6\n",
    "            elif centroid.y <= 40.5:\n",
    "                nj_longlat_assignment.iloc[index,nj_longlat_assignment.columns.get_loc('District')] = 8\n",
    "            elif centroid.y <= 40.9:\n",
    "                nj_longlat_assignment.iloc[index,nj_longlat_assignment.columns.get_loc('District')] = 10\n",
    "            else:\n",
    "                nj_longlat_assignment.iloc[index,nj_longlat_assignment.columns.get_loc('District')] = 12\n",
    "    except KeyError: \n",
    "        pass\n",
    "#print(nh_longitude_assignment)\n",
    "nj_longlat_assignment.to_csv('Recitation maps/nj_longlat_map.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb00508f-3df2-487c-90ac-56a7af050c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compactness of this Longlat assignment\n",
    "for district in range(1,13):\n",
    "    print(\"D\"+str(district)+\" PP : \"+str(pp_compactness(getDistrictShape(district,nj_longlat_assignment,nj_precinct_boundaries))))\n",
    "    print(\"D\"+str(district)+\" BR : \"+str(box_reock_compactness(getDistrictShape(district,nj_longlat_assignment,nj_precinct_boundaries))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c674a-6122-4440-a87a-9b5b4250f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# District Population of this longlat assignment\n",
    "print(getDistrictPopulations(nj_longlat_assignment,nj_precinct_data, 12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f943977-f3a0-41b8-930e-6e26d74092f2",
   "metadata": {},
   "source": [
    "# Now create your own redistricting maps\n",
    "Remember to check for contiguity, and to ensure that the population of the districts are balanced (which is not the case in the example above.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01353e8b",
   "metadata": {},
   "source": [
    "# Fair Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grading Rubric\n",
    "Maps produced **(2 maps, 6 pts per map)**\n",
    " - Maps in correct CSV format **(2pts)**\n",
    " - Districts are contigous **(1pts)**\n",
    " - Population is balanced **(1pts)**\n",
    " - Map is optimized for fairness/unfairness - description of how fairness/unfairness is computed is given in the report **(2pts)**\n",
    " \n",
    "Report **(6 points)**\n",
    "- Screenshots of the maps in DRA are included in the report **(1pts)**\n",
    "- In depth description and analysis of each map **(4pts)**\n",
    "    - What are their advantages?\n",
    "    - What are their drawbacks?\n",
    "    - Who is advantaged or penalized by the map?\n",
    "    - Would the map be a reasonable option for adoption by the legislature?\n",
    "- How would you imrpove your maps if you had more time? or, what other approach would you have tried. **(1pt)**\n",
    "\n",
    "Code provided **(2 points)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253e9bfc",
   "metadata": {},
   "source": [
    "## From Existing Map - Flip Step\n",
    "random based strategy similar to one in recitation_redistricting notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_precincts(current_map, district_from, district_to, num_flip, contiguity_data, precinct_data):\n",
    "    border_precincts = []\n",
    "    current_list = current_map[current_map['District']==district_from]['GEOID20']\n",
    "    for precinct in current_list:\n",
    "            neighbors = ast.literal_eval(\n",
    "                contiguity_data[contiguity_data['Precinct'] == precinct]['Neighbors'].values.tolist()[0])\n",
    "            for neighbor in neighbors:\n",
    "                if current_map.loc[current_map['GEOID20'] == neighbor, 'District'].values.tolist()[\n",
    "                    0] == district_to:\n",
    "                    border_precincts.append(precinct)\n",
    "                    \n",
    "    flipped_precincts = np.random.choice(border_precincts, num_flip)\n",
    "    for flip in flipped_precincts:\n",
    "        current_map.loc[current_map['GEOID20'] == flip, 'District'] = district_to\n",
    "        # check if we broke contiguity and revert the flip if we did\n",
    "        if (not isDistrictContiguous(district_from, current_map, contiguity_data) or not isDistrictContiguous(district_to, current_map, contiguity_data)):\n",
    "            current_map.loc[current_map['GEOID20'] == flip, 'District'] = district_from\n",
    "            print(\"Contiguity broken \" + flip)\n",
    "        else:\n",
    "            print(\"Went through \" + flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nj_flipstep_assignment = nj_current_assignment.copy()\n",
    "\n",
    "# Simplistic Approach\n",
    "\n",
    "flip_precincts(nj_flipstep_assignment, 1, 2, 8, nj_contiguity, nj_precinct_data)\n",
    "flip_precincts(nj_flipstep_assignment, 2, 1, 8, nj_contiguity, nj_precinct_data)\n",
    "\n",
    "flip_precincts(nj_flipstep_assignment, 2, 3, 8, nj_contiguity, nj_precinct_data)\n",
    "flip_precincts(nj_flipstep_assignment, 3, 2, 8, nj_contiguity, nj_precinct_data)\n",
    "\n",
    "for district in range(1, 12):\n",
    "  contiguous = isDistrictContiguous(district, nj_flipstep_assignment, nj_contiguity)\n",
    "  print(\"District\", district, \"Contiguous?\", contiguous)\n",
    "\n",
    "nj_flipstep_assignment.to_csv('Recitation maps/test.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fda189-958e-4639-8c9e-78d618ea055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous code not functionl\n",
    "#import random\n",
    "\n",
    "# nj_flip_assignment = nj_current_assignment.copy()\n",
    "\n",
    "# D1_Border_precincts = [] #NJ has 12 districts, and there should be 6361 precincts (rows)\n",
    "# D1_list = nj_flip_assignment[nj_flip_assignment['District']==1]['GEOID20'].tolist()\n",
    "\n",
    "\n",
    "# for precinct in D1_list: \n",
    "#     neighbors = nj_contiguity[nj_contiguity['GEOID20']==precinct]['Neighbor'].tolist()\n",
    "#     for neighbor in neighbors:\n",
    "#         if nj_flip_assignment[nj_flip_assignment['GEOID20']==neighbor]['District'].tolist()[0] == 2:\n",
    "#             #if one of the neighbor is in D2, then this is a border district\n",
    "#             D1_Border_precincts.append(precinct)\n",
    "#             break\n",
    "\n",
    "# # calculate percenage of total number of precincts\n",
    "# num_precincts_to_flip = int(len(D1_list) * 0.15)\n",
    "\n",
    "# for i in range(10):\n",
    "#     precinct_to_flip = np.random.choice(D1_Border_precincts)\n",
    "#     nj_flip_assignment.loc[nj_flip_assignment['GEOID20']==precinct_to_flip, 'District'] = 2\n",
    "#     if not isDistrictContiguous(nj_flip_assignment, 1) or not isDistrictContiguous(nj_flip_assignment, 2):\n",
    "#         nj_flip_assignment.loc[nj_flip_assignment['GEOID20']==precinct_to_flip, 'District'] = 1\n",
    "#         print(\"Contiguity was broken, precinct was reassigned back to D1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e9a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nj_flipstep_assignment = nj_current_assignment.copy()\n",
    "\n",
    "## Simplistic Approach \n",
    "## randomly select certain number D1 precincts that border D2 and assign them to D2\n",
    "\n",
    "\n",
    "# THIS CODE WORKS BUT IM GONNA COMMENT IT OUT FOR NOW\n",
    "# D1_border_precincts = []\n",
    "\n",
    "# D1_list = nj_current_assignment[nj_current_assignment['District']==1]['GEOID20'] \n",
    "# for precinct in D1_list:\n",
    "#   neighbors = ast.literal_eval(nj_contiguity[nj_contiguity['Precinct']==precinct]['Neighbors'].values.tolist()[0])\n",
    "#   for neighbor in neighbors:\n",
    "#     if nj_current_assignment.loc[nj_current_assignment['GEOID20']==neighbor,'District'].values.tolist()[0]==2:\n",
    "#       D1_border_precincts.append(precinct)\n",
    "#       break\n",
    "\n",
    "# flipped_precincts = np.random.choice(D1_border_precincts,8)\n",
    "# for flip in flipped_precincts:\n",
    "#     nj_flipstep_assignment.loc[nj_flipstep_assignment['GEOID20']==flip,'District']=2\n",
    "#     # check if we broke contiguity and revert the flip if we did\n",
    "#     if(isDistrictContiguous(1,nj_flipstep_assignment,nj_contiguity) is False or isDistrictContiguous(2,nj_flipstep_assignment,nj_contiguity) is False):\n",
    "#         nj_flipstep_assignment.loc[nj_flipstep_assignment['GEOID20']==flip,'District']=1\n",
    "#         print(\"Contiguity broken \" + flip)\n",
    "#     else:\n",
    "#         populations = getDistrictPopulations(nj_flipstep_assignment,nj_precinct_data, 12)\n",
    "#         pop1 = populations[1]\n",
    "#         pop2 = populations[2]\n",
    "#         if (abs(pop1 - pop2) / (pop1 + pop2) / 2) > 0.05:\n",
    "#             nj_flipstep_assignment.loc[nj_flipstep_assignment['GEOID20']==flip,'District']=1\n",
    "#             print(\"Population deviation too high \" + flip)\n",
    "\n",
    "# D2_border_precincts = []\n",
    "\n",
    "# D2_list = nj_current_assignment[nj_current_assignment['District']==2]['GEOID20'] \n",
    "# for precinct in D2_list:\n",
    "#   neighbors = ast.literal_eval(nj_contiguity[nj_contiguity['Precinct']==precinct]['Neighbors'].values.tolist()[0])\n",
    "#   for neighbor in neighbors:\n",
    "#     if nj_current_assignment.loc[nj_current_assignment['GEOID20']==neighbor,'District'].values.tolist()[0]==2:\n",
    "#       D2_border_precincts.append(precinct)\n",
    "#       break\n",
    "\n",
    "# flipped_precincts = np.random.choice(D2_border_precincts,8)\n",
    "# for flip in flipped_precincts:\n",
    "#     nj_flipstep_assignment.loc[nj_flipstep_assignment['GEOID20']==flip,'District']=1\n",
    "#     # check if we broke contiguity and revert the flip if we did\n",
    "#     if(isDistrictContiguous(1,nj_flipstep_assignment,nj_contiguity) is False or isDistrictContiguous(2,nj_flipstep_assignment,nj_contiguity) is False):\n",
    "#         nj_flipstep_assignment.loc[nj_flipstep_assignment['GEOID20']==flip,'District']=2\n",
    "#         print(\"Contiguity broken \" + flip)\n",
    "#     else:\n",
    "#         populations = getDistrictPopulations(nj_flipstep_assignment,nj_precinct_data, 12)\n",
    "#         pop1 = populations[1]\n",
    "#         pop2 = populations[2]\n",
    "#         if (abs(pop1 - pop2) / (pop1 + pop2) / 2) > 0.05:\n",
    "#             nj_flipstep_assignment.loc[nj_flipstep_assignment['GEOID20']==flip,'District']=1\n",
    "#             print(\"Population deviation too high \" + flip)\n",
    "\n",
    "# Now 2 and 3\n",
    "# D2_border_precincts = []\n",
    "\n",
    "# D2_list = nj_flipstep_assignment[nj_flipstep_assignment['District']==2]['GEOID20'] \n",
    "# for precinct in D2_list:\n",
    "#   neighbors = ast.literal_eval(nj_contiguity[nj_contiguity['Precinct']==precinct]['Neighbors'].values.tolist()[0])\n",
    "#   for neighbor in neighbors:\n",
    "#     if nj_flipstep_assignment.loc[nj_flipstep_assignment['GEOID20']==neighbor,'District'].values.tolist()[0]==2:\n",
    "#       D2_border_precincts.append(precinct)\n",
    "#       break\n",
    "\n",
    "# flipped_precincts = np.random.choice(D2_border_precincts,8)\n",
    "# for flip in flipped_precincts:\n",
    "#     nj_flipstep_assignment.loc[nj_flipstep_assignment['GEOID20']==flip,'District']=3\n",
    "#     # check if we broke contiguity and revert the flip if we did\n",
    "#     if(isDistrictContiguous(1,nj_flipstep_assignment,nj_contiguity) is False or isDistrictContiguous(2,nj_flipstep_assignment,nj_contiguity) is False):\n",
    "#         nj_flipstep_assignment.loc[nj_flipstep_assignment['GEOID20']==flip,'District']=2\n",
    "#         print(\"Contiguity broken \" + flip)\n",
    "#     else:\n",
    "#         populations = getDistrictPopulations(nj_flipstep_assignment,nj_precinct_data, 12)\n",
    "#         pop1 = populations[1]\n",
    "#         pop2 = populations[2]\n",
    "#         if (abs(pop1 - pop2) / (pop1 + pop2) / 2) > 0.05:\n",
    "#             nj_flipstep_assignment.loc[nj_flipstep_assignment['GEOID20']==flip,'District']=3\n",
    "#             print(\"Population deviation too high \" + flip)\n",
    "\n",
    "        \n",
    "# Sample precints and flip them UNLESS it breaks contiguity\n",
    "# flipped_precincts = np.random.choice(D1_border_precincts,5)\n",
    "\n",
    "# for flip in flipped_precincts:\n",
    "#     nj_flipstep_assignment.loc[nj_flipstep_assignment['GEOID20']==flip,'District']=2\n",
    "#     #check if we broke contiguity and revert the flip if we did\n",
    "#     if(isDistrictContiguous(1,nj_flipstep_assignment,nj_contiguity) is False or isDistrictContiguous(2,nj_flipstep_assignment,nj_contiguity) is False):\n",
    "#         nj_flipstep_assignment.loc[nj_flipstep_assignment['GEOID20']==flip,'District']=1\n",
    "#         print(\"Contiguity broken \" + flip)\n",
    "    # else:\n",
    "    #     populations = getDistrictPopulations(nj_flipstep_assignment,nj_precinct_data, 5)\n",
    "    #     pop1 = populations[1]\n",
    "    #     pop2 = populations[2]\n",
    "    #     if (abs(pop1 - pop2) / (pop1 + pop2) / 2) > 0.05:\n",
    "    #         nj_flipstep_assignment.loc[nj_flipstep_assignment['GEOID20']==flip,'District']=1\n",
    "    #         print(\"Population deviation too high \" + flip)\n",
    "\n",
    "# First we randomly select 5 DIFFERENT D2 precincts that border D1 and assign them to D1\n",
    "# D2_border_precincts = []\n",
    "# D2_list = nj_longlat_assignment[nj_longlat_assignment['District']==2]['GEOID20']\n",
    "# for precinct in D2_list:\n",
    "#     neighbors = ast.literal_eval(nj_contiguity[nj_contiguity['Precinct']==precinct]['Neighbors'].values.tolist()[0])\n",
    "#     for neighbor in neighbors:\n",
    "#         if nj_flipstep_assignment.loc[nj_flipstep_assignment['GEOID20']==neighbor,'District'].values.tolist()[0]==1 and precinct not in flipped_precincts:\n",
    "#             #if one of the neighbor is in D2, then this is a border district. We do not want to re-flip a district we just flipped\n",
    "#             D2_border_precincts.append(precinct)\n",
    "#             break\n",
    "\n",
    "#Sample 5 and flip them UNLESS it breaks contiguity\n",
    "# if len(D2_border_precincts) >= 5:\n",
    "#     flipped_precincts = np.random.choice(D2_border_precincts,5)\n",
    "# else:\n",
    "#     print(\"Not enough border precincts to flip\")\n",
    "#     flipped_precincts = []\n",
    "\n",
    "# print(\"Number of D2 border precincts:\", len(D2_border_precincts))\n",
    "# print(\"D2 border precincts:\", D2_border_precincts)\n",
    "\n",
    "\n",
    "# for flip in flipped_precincts:\n",
    "#     nj_flipstep_assignment.loc[nj_flipstep_assignment['GEOID20']==flip,'District']=1\n",
    "#     #check if we broke contiguity and revert the flip if we did\n",
    "#     if(isDistrictContiguous(1,nj_flipstep_assignment,nj_contiguity) is False or isDistrictContiguous(2,nj_flipstep_assignment,nj_contiguity) is False):\n",
    "#         nj_flipstep_assignment.loc[nj_flipstep_assignment['GEOID20']==flip,'District']=2\n",
    "#         print(\"Contiguity broken \" + flip)\n",
    "#     else:\n",
    "#         populations = getDistrictPopulations(nj_flipstep_assignment,nj_precinct_data, 12)\n",
    "#         pop1 = populations[1]\n",
    "#         pop2 = populations[2]\n",
    "#         if (abs(pop1 - pop2) / (pop1 + pop2) / 2) > 0.05:\n",
    "#             nj_flipstep_assignment.loc[nj_flipstep_assignment['GEOID20']==flip,'District']=2\n",
    "#             print(\"Population deviation too high \" + flip)\n",
    "\n",
    "\n",
    "# for district in range(1, 12):\n",
    "#   contiguous = isDistrictContiguous(district, nj_flipstep_assignment, nj_contiguity)\n",
    "#   print(\"District\", district, \"Contiguous?\", contiguous)\n",
    "\n",
    "# print(len(D1_border_precincts))\n",
    "# print(len(D2_border_precincts))\n",
    "# print(flipped_precincts) \n",
    "# print(len(set(flipped_precincts))) # check unique\n",
    "\n",
    "# nj_flipstep_assignment.to_csv('Recitation maps/test.csv',index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8475ac82",
   "metadata": {},
   "source": [
    "# Unfair Map\n",
    "map should be designed to give a party/community/area some advantage (unfair map) using some creative gerrymandering techniques. The less likely the map would be to be thrown out of courts for obvious gerrymandering, the better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
